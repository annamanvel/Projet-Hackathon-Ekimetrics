{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8fa7a20",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecff085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:53:59.159825Z",
     "start_time": "2021-11-08T16:53:50.592927Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import sklearn\n",
    "import dalex as dx\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "{\n",
    "    \"numpy\": np.__version__,\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"matplotlib\": matplotlib.__version__,\n",
    "    \"seaborn\": sns.__version__,\n",
    "    \"sklearn\": sklearn.__version__,\n",
    "    \"dalex\": dx.__version__,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94117841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./stackoverflow_full.csv\", index_col=0)\n",
    "target = \"Employed\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fa6df8a",
   "metadata": {},
   "source": [
    "## ML prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9779cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split your data set in 2 parts : training and testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=target),\n",
    "    df[target],\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a206708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protected attribute is 0 if a man or non binary and 0 if a woman plus the age\n",
    "\n",
    "protected = (pd.Series(np.where(X_test[\"Gender\"] == \"Woman\", '1', '0'), index=X_test.index) \n",
    "             + '_' \n",
    "             + X_test.Age)\n",
    "protected_train = (pd.Series(np.where(X_train[\"Gender\"] == \"Woman\", '1', '0').astype(str), index=X_train.index) \n",
    "                   + '_' \n",
    "                   + X_train.Age)\n",
    "\n",
    "# Privileged population is men under 35 years old\n",
    "privileged = '0_<35'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d72271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "      (\"passthrough\", make_column_selector(dtype_include=np.number)),\n",
    "      (OneHotEncoder(handle_unknown=\"ignore\"), make_column_selector(dtype_include=object))\n",
    ")\n",
    "\n",
    "#You can change the Decision tree hyperparameters or the classifier below\n",
    "\n",
    "clf_decisiontree = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=10, random_state=123))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ecf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_decisiontree.fit(df.drop(columns=[target]), df[target])\n",
    "clf_decisiontree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962fdd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_decisiontree = dx.Explainer(clf_decisiontree, df.drop(columns=[target]), df[target], verbose=False)\n",
    "exp_decisiontree = dx.Explainer(clf_decisiontree, X_test, y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d63024",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_decisiontree.model_performance().result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b646be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree = exp_decisiontree.model_fairness(protected=protected, privileged=privileged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38caae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.fairness_check(epsilon = 0.8) # default epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot(verbose=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bc19112",
   "metadata": {},
   "source": [
    "### Strategy 1.2 Pre-processing: Remove sensitive attribute\n",
    "\n",
    "The first thing that can come to mind is to remove sensitive variables. However, this option is considered as really naive since it can have no effect. \n",
    "\n",
    "**When may this method works ?**\n",
    "\n",
    "➤ If the sensitive variable conveys the bias (mostly) on its own.\n",
    "\n",
    "    ➤ This means, the sensitive variable is correlated with the target variable\n",
    "    ➤ This also means, the sensitive variable is NOT correlated at all with other explanatory variables and any combination of other explanatory variables cannot be used as a proxi for the sensitive variable. \n",
    "\n",
    "Most of the time, the bias is shared accross explanatory variables. For example, a bias on gender may be present in many ones (salary, education, socio-professional category, etc.)\n",
    "\n",
    "This transformation is not possible in the dalex module (since it not a recommended option to deal with bias). To implement it, a new model has to be trained and explained. This time the sensitive variable\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain a model without sensitive variables \"age\" and \"sex\"\n",
    "X_train_restricted = X_train.drop(['Gender', 'Age'], axis=1)\n",
    "\n",
    "preprocessor_restr = make_column_transformer(\n",
    "      (\"passthrough\", make_column_selector(dtype_include=np.number)),\n",
    "      (OneHotEncoder(handle_unknown=\"ignore\"), make_column_selector(dtype_include=object))\n",
    ")\n",
    "\n",
    "clf_decisiontree_restr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_restr),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=7, random_state=123))\n",
    "])\n",
    "\n",
    "clf_decisiontree_restr.fit(X_train_restricted, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0215a2d9",
   "metadata": {},
   "source": [
    "#### Algorithmic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601caa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dalex explainer for the model without sensitive variables\n",
    "exp_decisiontree_restr = dx.Explainer(clf_decisiontree_restr, X_test, y_test, verbose=True)\n",
    "\n",
    "exp_decisiontree_restr.model_performance().result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed34616f",
   "metadata": {},
   "source": [
    "\n",
    "When comparing these performance metrics with those of the model with all variables, what can you observe?\n",
    "\n",
    "In our case, can we use this metric to compare models?\n",
    "\n",
    "#### Fairness performance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54298d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see effect of removing sensitive variables on bias metrics\n",
    "fairness_decisiontree_restr = exp_decisiontree_restr.model_fairness(protected=protected, privileged=privileged, \n",
    "                                                                    label='DecisionTreeClassifier_no_sensitive')\n",
    "\n",
    "fairness_decisiontree_restr.fairness_check(epsilon = 0.8) # default epsilon\n",
    "\n",
    "fairness_decisiontree_restr.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f57210c",
   "metadata": {},
   "source": [
    "What impact does the pre-processing startegy had on the bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 2 (or more) fairness objects in dalex (add them as list in parameters). \n",
    "# It's also possible to choose the plot type ! \n",
    "fairness_decisiontree_restr.plot([fairness_decisiontree], type='radar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
